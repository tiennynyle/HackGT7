{"ast":null,"code":"var _jsxFileName = \"/Users/thotra/Desktop/hack-gt/src/components/Dictaphone.jsx\";\nimport React, { useEffect } from \"react\";\nimport SpeechRecognition, { useSpeechRecognition } from \"react-speech-recognition\";\nimport { useVoiceCommands } from \"../data/useVoiceCommands\";\nimport { MessageContext } from \"../Context\";\nexport const Dictaphone = () => {\n  const {\n    items\n  } = React.useContext(MessageContext);\n  console.log(items);\n  const {\n    transcript\n  } = useSpeechRecognition();\n  useVoiceCommands({\n    transcript\n  });\n  useEffect(() => {\n    SpeechRecognition.startListening({\n      continuous: true\n    });\n  }, []);\n  useEffect(() => {\n    if (transcript.endsWith(\"please\")) {\n      SpeechRecognition.stopListening();\n    }\n  }, [transcript]);\n\n  if (!SpeechRecognition.browserSupportsSpeechRecognition()) {\n    return null;\n  }\n\n  return /*#__PURE__*/React.createElement(\"div\", {\n    __self: this,\n    __source: {\n      fileName: _jsxFileName,\n      lineNumber: 30,\n      columnNumber: 5\n    }\n  }, /*#__PURE__*/React.createElement(\"p\", {\n    __self: this,\n    __source: {\n      fileName: _jsxFileName,\n      lineNumber: 31,\n      columnNumber: 7\n    }\n  }, transcript));\n};","map":{"version":3,"sources":["/Users/thotra/Desktop/hack-gt/src/components/Dictaphone.jsx"],"names":["React","useEffect","SpeechRecognition","useSpeechRecognition","useVoiceCommands","MessageContext","Dictaphone","items","useContext","console","log","transcript","startListening","continuous","endsWith","stopListening","browserSupportsSpeechRecognition"],"mappings":";AAAA,OAAOA,KAAP,IAAgBC,SAAhB,QAAiC,OAAjC;AACA,OAAOC,iBAAP,IACEC,oBADF,QAEO,0BAFP;AAGA,SAASC,gBAAT,QAAiC,0BAAjC;AACA,SAASC,cAAT,QAA+B,YAA/B;AAEA,OAAO,MAAMC,UAAU,GAAG,MAAM;AAC9B,QAAM;AAAEC,IAAAA;AAAF,MAAYP,KAAK,CAACQ,UAAN,CAAiBH,cAAjB,CAAlB;AACAI,EAAAA,OAAO,CAACC,GAAR,CAAYH,KAAZ;AAEA,QAAM;AAAEI,IAAAA;AAAF,MAAiBR,oBAAoB,EAA3C;AAEAC,EAAAA,gBAAgB,CAAC;AAAEO,IAAAA;AAAF,GAAD,CAAhB;AACAV,EAAAA,SAAS,CAAC,MAAM;AACdC,IAAAA,iBAAiB,CAACU,cAAlB,CAAiC;AAAEC,MAAAA,UAAU,EAAE;AAAd,KAAjC;AACD,GAFQ,EAEN,EAFM,CAAT;AAIAZ,EAAAA,SAAS,CAAC,MAAM;AACd,QAAIU,UAAU,CAACG,QAAX,CAAoB,QAApB,CAAJ,EAAmC;AACjCZ,MAAAA,iBAAiB,CAACa,aAAlB;AACD;AACF,GAJQ,EAIN,CAACJ,UAAD,CAJM,CAAT;;AAMA,MAAI,CAACT,iBAAiB,CAACc,gCAAlB,EAAL,EAA2D;AACzD,WAAO,IAAP;AACD;;AAED,sBACE;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,kBACE;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,KAAIL,UAAJ,CADF,CADF;AAKD,CA1BM","sourcesContent":["import React, { useEffect } from \"react\";\nimport SpeechRecognition, {\n  useSpeechRecognition,\n} from \"react-speech-recognition\";\nimport { useVoiceCommands } from \"../data/useVoiceCommands\";\nimport { MessageContext } from \"../Context\";\n\nexport const Dictaphone = () => {\n  const { items } = React.useContext(MessageContext);\n  console.log(items);\n\n  const { transcript } = useSpeechRecognition();\n\n  useVoiceCommands({ transcript });\n  useEffect(() => {\n    SpeechRecognition.startListening({ continuous: true });\n  }, []);\n\n  useEffect(() => {\n    if (transcript.endsWith(\"please\")) {\n      SpeechRecognition.stopListening();\n    }\n  }, [transcript]);\n\n  if (!SpeechRecognition.browserSupportsSpeechRecognition()) {\n    return null;\n  }\n\n  return (\n    <div>\n      <p>{transcript}</p>\n    </div>\n  );\n};\n"]},"metadata":{},"sourceType":"module"}